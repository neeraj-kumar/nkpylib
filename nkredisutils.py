"""A set of utilities to use for the Redis database.

Licensed under the 3-clause BSD License:

Copyright (c) 2010, Neeraj Kumar (neerajkumar.org)
All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are met:
    * Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.
    * Redistributions in binary form must reproduce the above copyright
      notice, this list of conditions and the following disclaimer in the
      documentation and/or other materials provided with the distribution.
    * Neither the name of the author nor the
      names of its contributors may be used to endorse or promote products
      derived from this software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
DISCLAIMED. IN NO EVENT SHALL NEERAJ KUMAR BE LIABLE FOR ANY
DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
"""

from __future__ import with_statement
import os, sys, time
from nkutils import *
import redis
from pprint import pprint, pformat

PPRINT_WIDTH = getConsoleSize()[0]


# SMALL UTILS
def typedget(cast, db, key, field=None, default=None):
    """Returns the value of the given key (and field if a hash) as the given type.
    If not found, returns the given default (None as default)."""
    try:
        if field:
            return cast(db.hget(key, field))
        else:
            return cast(db.get(key))
    except TypeError:
        return default

def intget(*args, **kw):
    """Returns the value of the given key (and field if a hash) as an int."""
    return typedget(int, *args, **kw)

def floatget(*args, **kw):
    """Returns the value of the given key (and field if a hash) as an float."""
    return typedget(float, *args, **kw)

def keyspec(db, keyspec):
    """Returns a set of keys from the given keyspec.
    This can be one of:
        (fmt, vals) - keys are generated by fmt % val
        list - assumed to be list of keys; returned as-is
        string - passed to db.keys() to get list
    """
    # wildcard
    if isinstance(keyspec, basestring): return db.keys(keyspec)
    # fmt + vals
    if len(keyspec) == 2 and not isinstance(keyspec[1], basestring):
        keys = [keyspec[0] % v for v in keyspec[1]]
        return keys
    # everything else - just return as-is
    return keyspec

def gethashdict(db, key, fields):
    """Returns a dict from the hash at the given key and for the given fields."""
    return dict(fields, db.hmget(key, fields))

def iterhash(db, keys, fields, castfuncs=None):
    """Iterates through a set of hashvalues for a given set of keys.
    The `keys` are passed to keyspec() to get actual list of keys.
    The fields can be one of:
        single string: returns single string as output, per key
        list of strings: returns list of strings per key
        dict of strings->castfunc: returns dict of casted vals per key
    Returns (keys, retvals), where the latter is as above.
    """
    keys = keyspec(db, keys)
    if isinstance(fields, basestring): # get single value
        vals = pipefunc(db, keys, 'hget', fields)
        if castfuncs:
            vals = map(castfuncs, vals)
    elif isinstance(fields, dict): # dict of outputs
        fields, funcs = zip(*sorted(fields.iteritems()))
        vals = pipefunc(db, keys, 'hmget', fields)
        vals = [[func(v) for func, v in zip(funcs, row)] for row in vals]
        vals = [dict(zip(fields, row)) for row in vals]
    else: # list of fields
        vals = pipefunc(db, keys, 'hmget', fields)
        if castfuncs:
            if callable(castfuncs): # single casting function
                vals = [map(castfuncs, row) for row in vals]
            else: # multiple casting functions
                assert len(castfuncs) == len(fields)
                vals = [[func(v) for v in row] for row in vals]
    return (keys, vals)

def idFromHash(db, key, hashname, counter):
    """Tries to get the id for 'key' from 'hashname'.
    If it doesn't exist, increments 'counter' and sets the id
    in the hash, and then finally returns the id.
    Note that the id is NOT cast into an int."""
    id = db.hget(hashname, key)
    if id is None:
        id = db.incr(counter)
        db.hset(hashname, key, id)
    return id

def makeqname(prefix=''):
    """Makes the qname() function with the given prefix"""
    def qname(*args):
        """Returns a qname from the given list of args, separated by :"""
        if prefix:
            els = [prefix] + list(args)
        else:
            els = list(args)
        return getListAsStr(els, ':')
    return qname

def workername():
    """Returns a new random worker name"""
    import uuid
    return str(uuid.uuid1())

def pipefunc(db, keys, funcname, *args, **kw):
    """Runs a pipeline with the given funcname on each given key.
    This function is equivalent to:
        [db.funcname(k, *args, **kw) for k in keys]
    """
    p = db.pipeline()
    func = getattr(p, funcname)
    [func(k, *args, **kw) for k in keys]
    return p.execute()

def kpipefunc(db, keys, funcname, *args, **kw):
    """Runs pipefunc in a zip with the keys.
    Returns pairs of (key, pipefunc(key))"""
    keylist = list(keys)
    return zip(keylist, pipefunc(db, keylist, funcname, *args, **kw))

def pipefuncs(db, keys, funcs):
    """Runs a pipeline with the given functions on each given key.
    Note that each func must take exactly (key, pipeline) as args.
    This is more annoying than pipefunc(), but sometimes this function is more useful.
    This function is equivalent to:
        [(funcs[0](k, db), funcs[1](k, db), ...) for k in keys]
    """
    p = db.pipeline()
    for k in keys:
        for f in funcs:
            f(k, p)
    vals = p.execute()
    ret = list(grouper(len(funcs), vals))
    return ret

def pipedec(db):
    """Decorator for pipelining multiple redis gets/sets in live code.
    WARNING: THIS IS VERY FRAGILE -- USE WITH EXTREME CAUTION!!!

    Redis pipelines are great when only setting stuff, or when getting
    the same value for lots of keys, but if you're doing a mix of things,
    then it becomes annoying to separate out pipelines() for gets.
    Often, you end up writing essentially the same code twice -- the first time
    making all the pipeline calls, then calling execute(), then a second time
    setting all the values you want to.

    This decorator aims to simplify this process for common cases, by essentially
    automating the above process.

    Assumptions: your function MUST adhere to the following guidelines:
        - Your code must be idempotent to being run twice. This includes:
            * deterministic ordering (don't want to assign wrong values to wrong items!)
            * allowing values to be set multiple times (first time will be the pipeline object)
            * no list appending (since the first time will be the pipeline object)
            * no dependencies between local variables that depend on redis-assigned values
        - NO redis-dependent branches. The first time through there will be no values -- only pipelines.

    Here is an example.

    @pipedec(db)
    def simple(p):
        v = p.hget('key', 'field')
        ret = p.hmset('k2', 'f', 42)
        a = p.smembers('skey')
        return ret

    # this becomes the following

    p = db.pipeline()

    v = p.hget('key', 'field')
    ret = p.hmset('k2', 'f', 42)
    a = p.smembers('skey')
    # return value ignored

    vals = p.execute()

    v = vals.pop(0)
    ret = vals.pop(0)
    a = vals.pop(0)
    return ret
    """
    #TODO see if this function is actually useful, given all these restrictions
    redisfuncs = 'append blpop brpop brpoplpush config_get config_set dbsize decr delete echo exists expire expireat flushall flushdb get getbit getset hdel hexists hget hgetall hincrby hkeys hlen hmget hmset hset hsetnx hvals incr info keys lastsave lindex linsert llen lpop lpush lpushx lrange lrem lset ltrim mget move mset msetnx object persist ping publish randomkey rename renamenx rpop rpoplpush rpush rpushx sadd save scard sdiff sdiffstore set setbit setex setnx setrange sinter sinterstore sismember smembers smove sort spop srandmember srem strlen substr sunion sunionstore ttl type zadd zcard zcount zincrby zinterstore zrange zrangebyscore zrank zrem zremrangebyrank zremrangebyscore zrevrange zrevrangebyscore zrevrank zscore zunionstore'.split()
    def ret(fn):
        def newfn(*args, **kw):
            p = db.pipeline()
            times = [time.time()]
            fn(p, *args, **kw)
            print 'finished first call'
            times.append(time.time())
            vals = p.execute()
            print 'finished execute'
            times.append(time.time())
            class ValueWrapper(object):
                """A wrapper over some values that simply returns them in order, no matter the method called."""
                def __init__(self, vals):
                    self.vals = vals
                    for f in redisfuncs:
                        setattr(self, f, self.popnext)

                def popnext(self, *args, **kw):
                    """Returns the next value"""
                    print 'In popnext with args %s, kw %s' % (args, kw)
                    return self.vals.pop(0)

            ret = fn(ValueWrapper(vals), *args, **kw)
            times.append(time.time())
            print 'Got times %s' % (getTimeDiffs(times))
            return ret
        newfn.__name__ = fn.__name__ + ' (pipeline decorated)'
        return newfn
    return ret


# INTERACTIVE/LARGER UTILS
def dbinfo(db, *patterns, **kw):
    """Gets types and lengths of keys matching given patterns.
    If any pattern is a list, then assumes it's a list of keys.
    Else, assumes it's a string and expands out using db.keys().
    If no patterns are given, then processes all keys.

    The lengths are defined by redis type as:
        string: strlen
        hash: hlen
        set: scard
        zset: zcard
        list: llen
    You can optionally give these kw params:
        'outf': a stream (default: sys.stdout) to pretty print things on.
        'detailed': if true (default: 0), then print the full items as well.

    Returns a dict mapping keys to (type, length) if not detailed,
    or (type, length, values) if detailed=1.
    """
    from pprint import pformat
    outf = kw.get('outf', sys.stdout)
    detailed = kw.get('detailed', 0)
    if patterns:
        keys = []
        for p in patterns:
            if isinstance(p, basestring):
                keys.extend(db.keys(p))
            else:
                keys.extend(p)
        keys.sort()
    else:
        keys = sorted(db.keys())
    types = pipefunc(db, keys, 'type')
    p = db.pipeline()
    if detailed:
        # get detailed values
        vfuncs = dict(string=p.get, hash=p.hgetall, zset=lambda k: p.zrange(k, 0, -1, withscores=1), set=p.smembers, list=lambda k: p.lrange(k, 0, -1))
        [vfuncs[t](k) for k, t in zip(keys, types)]
        vals = p.execute()
        # in this case, the simple values are just the lengths of each thing
        lengths = [len(v) for v in vals]
    else:
        # lengths only
        lfuncs = dict(string=p.strlen, hash=p.hlen, zset=p.zcard, set=p.scard, list=p.llen)
        [lfuncs[t](k) for k, t in zip(keys, types)]
        lengths = p.execute()
    # now print things out
    ret = {}
    typestrs = dict(string='STR ', hash='HASH', zset='ZSET', set='SET ', list='LIST')
    for k, t in zip(keys, types):
        l = lengths.pop(0)
        v = vals.pop(0) if detailed else None
        if outf:
            print >>outf, '%s %7d %s' % (typestrs[t], l, k)
            if detailed:
                s = blockindent(pformat(v), indent=' '*14, initial=' '*14)
                print >>outf, s
        ret[k] = (t, l, v) if detailed else (t, l)
    return ret

def dblist(db, *patterns, **kw):
    """Simple wrapper for dbinfo() with detailed=1"""
    kw['detailed'] = 1
    return dbinfo(db, *patterns, **kw)

def keysAtLevel(db, level, dlm=':'):
    """Returns the set of keys at the given "level".
    This is defined as the number of elements of the key, when split by 'dlm'.
    """
    level = int(level)
    ret = [k for k in db.keys() if len(k.split(dlm)) == level]
    return ret

def keysize(db, k):
    """Returns the estimate size, in bytes, of a key and its value.
    The size of the key is simply its length.
    For values, it depends on the datatype:
        For strings, it's the length of the string
        For hashes, it's the sum of the lengths of the fields and their values
        For lists and sets, it's the sum of the lengths of each item
        For sorted sets, it's the 8*num (for the scores) + length of each member
    Returns 0 if the key is not found
    """
    if not k: return 0
    t = db.type(k)
    if not t: return 0
    ret = len(k)
    if t == 'string':
        ret += db.strlen(k)
    elif t == 'hash':
        d = db.hgetall(k)
        for f, v in d.iteritems():
            ret += len(f) + len(v)
    elif t == 'list':
        for el in db.lrange(k, 0, -1):
            ret += len(el)
    elif t == 'set':
        for el in db.smembers(k):
            ret += len(el)
    elif t == 'zset':
        for val in db.zrange(k, 0, -1):
            ret += 8 + len(val)
    return ret

def keymem(db, keys):
    """Returns detailed sizing information for the given keys.
    The return object is a dictionary mapping keys to memory dicts.
    Each memory dict contains:
        - keys: list of key names. This is needed for aggregation, but in this
          method, it simply contains a single element: the original key name.
        - types: list of types, corresponding to the list in 'keys'
        - length: the number of keys. Here, it's just 1.
        - keylen: sum of the lengths of the keynames.
        - num: length of sub-elements, if applicable. By type, this is:
            string: 1
            hash: hlen
            list: llen
            set: scard
            zset: zcard
        - skeylen: sum of the lengths of the subkeys. By type:
            string: 0
            hash: len(sum(hkeys))
            list: 0
            set: 0
            zset: len(sum(zrange))
        - svallen: sum of the lengths of the subvals. By type:
            string: strlen
            hash: len(sum(hvals))
            list: len(sum(lrange))
            set: len(sum(smembers))
            zset: 8*zcard
        - subtotal: skeylen + svallen
        - total: subtotal + keylen
    """
    # get types
    p = db.pipeline()
    [p.type(k) for k in keys]
    types = p.execute()
    # create return dicts
    default = dict(length=1, num=0, keylen=0, skeylen=0, svallen=0, subtotal=0, total=0)
    ret = [dict(keys=[k], types=[t], **default) for k, t in zip(keys, types)]
    ret = dict(zip(keys, ret))
    # partition keys by types
    parts, junk = partitionByFunc(zip(keys, types), lambda p: p[1])
    for type, ktpairs in parts.items():
        cur, junk = zip(*ktpairs)
        #print type, len(cur), cur[:5]
        pl = lambda funcname, *args: pipefunc(db, cur, funcname, *args)
        pll = lambda funcname, *args: [len(''.join(s)) for s in pl(funcname, *args)]
        # Build up lists of each variable type
        keylens = [len(k) for k in cur]
        if type == 'string':
            nums = [1] * len(cur)
            skeylens = [0] * len(cur)
            svallens = pl('strlen')
        elif type == 'hash':
            nums = pl('hlen')
            skeylens = pll('hkeys')
            svallens = pll('hvals')
        elif type == 'list':
            nums = pl('llen')
            skeylens = [0] * len(cur)
            svallens = pll('lrange', 0, -1)
        elif type == 'set':
            nums = pl('scard')
            skeylens = [0] * len(cur)
            svallens = pll('smembers')
        elif type == 'zset':
            nums = pl('zcard')
            skeylens = pll('zrange', 0, -1)
            svallens = pl('zcard')
        else:
            print 'Type is %s!' % (type)
            nums = [0] * len(cur)
            skeylens = [0] * len(cur)
            svallens = [0] * len(cur)
        # set all the values
        for el in zip(cur, keylens, nums, skeylens, svallens):
            k, rest = el[0], el[1:]
            #print '  ', k, rest
            for field, val in zip('keylen num skeylen svallen'.split(), rest):
                ret[k][field] += val
            ret[k]['subtotal'] = ret[k]['skeylen'] + ret[k]['svallen']
            ret[k]['total'] = ret[k]['subtotal'] + ret[k]['keylen']
    #pprint.pprint(ret)
    return ret

def aggrmem(mems):
    """Aggregates memory information returned from keymem().
    If mems is a dict, uses its values() only.
    If mems is a list, uses it directly.
    Returns a single memory structure, formatted just like keymem():
        - keys: list of key names
        - types: list of types, corresponding to the list in 'keys'
        - length: total number of keys
        - keylen: sum of the lengths of the keynames.
        - num: total length of sub-elements
        - skeylen: total sum of the lengths of the subkeys
        - svallen: total sum of the lengths of the subvals
        - subtotal: skeylen + svallen
        - total: subtotal + keylen
    """
    try:
        mems = mems.values()
    except Exception: pass
    ret = dict(keys=[], types=[], length=0, keylen=0, num=0, skeylen=0, svallen=0, subtotal=0, total=0)
    for d in mems:
        ret['keys'].extend(d['keys'])
        ret['types'].extend(d['types'])
        for k in 'length keylen num skeylen svallen subtotal total'.split():
            ret[k] += d[k]
    return ret

def groupkeys(keys, patterns):
    """Groups the given set of keys using the given patterns.
    It runs through the patterns sequentially, removing those from keys.
    Returns a dict with {pattern: [matching keys]}.
    Unmatches keys are added with None as the key."""
    from collections import defaultdict
    from fnmatch import fnmatch
    ret = defaultdict(list)
    for k in keys:
        matched = 0
        for p in patterns:
            if fnmatch(k, p):
                ret[p].append(k)
                matched = 1
                break
        if not matched:
            ret[None].append(k)
    return dict(**ret)

def groupedmem(db, keys, patterns):
    """Returns memory information for the given keys grouped by the given patterns"""
    groups = groupkeys(keys, patterns)
    ret = {}
    for p in patterns+[None]:
        cur = groups.get(p, None)
        if not cur: continue
        mem = keymem(db, cur)
        aggr = aggrmem(mem)
        ret[p] = aggr
    return ret


# REDIS-BASED MEMOIZATION
def makeredismemoize(prefix, host='127.0.0.1', port=6379, dbnum=0, timeout=60*60, flushold=0):
    """Function to create redis-based cache decorator.
    Make sure it's a functional method (i.e., no side effects).
    The first parameter is a prefix to use. Key names are found by adding the
    function name and then the args and kwargs.
    The next few parameters are database connection parameters.
    The timeout parameter sets the TTL of all cache entries, in secs (default 1 hour).
    If flushold is true, then it will flush all old keys beginning with the prefix.
    """
    import cPickle as pickle
    db = redis.Redis(host=host, port=port, db=dbnum)
    x = db.keys('a') # we need to make a call to make sure the database is actually accessible
    if flushold:
        todel = db.keys('%s-*' % prefix)
        db.delete(todel)
        print >>sys.stderr, 'Deleting all old keys starting with %s' % (prefix,)
    def actualret(fn):
        curprefix = '%s-%s-' % (prefix, fn.__name__)
        def newfn(*args, **kw):
            keybase = repr((tuple(args), tuple(sorted(kw.items()))))
            key = curprefix+keybase
            # first see if we have the value in redis
            ret = db.get(key)
            if ret: return pickle.loads(ret)
            print >>sys.stderr, 'Created redis memoize key %s\n%s' % (keybase, key)
            # we didn't have it, so compute it
            ret = fn(*args, **kw)
            db.set(key, pickle.dumps(ret, -1))
            db.expire(key, timeout)
            return ret
        newfn.__name__ = fn.__name__ + ' (REDIS MEMOIZED for %ss)' % (timeout,)
        return newfn
    return actualret


# REDIS-BASED QUEUES
class RedisQueueService(object):
    """A queue service which uses redis as the backing store.
    Every item queued MUST be of the form (id, item), where the id is unique."""

    def __init__(self, host='localhost', port=6379, db=0, password=None, timeout=1, retries=-1, socket_timeout=None, encoder='pickle'):
        """Initializes the queue service with the given parameters,
        most of which are directly passed to the Redis() constructor.
        The timeout parameter is used to determine the delay between retrying broken connections.
        The retries parameter can be used to limit the number of retries on the initial connection attempt.
            If retries < 0, then it retries forever.
            If retries > 0, then it retries that many times.
        The encoder sets the type of encoding to use. Options are:
            'pickle' [default]: pickle
            'json': json encoding
        """
        self.host = host
        self.port = port
        self.db = db
        self.password = password
        self.timeout = timeout
        self.socket_timeout = socket_timeout
        self.encoder = encoder
        assert self.encoder in 'pickle json'.split()
        self.resetredis(retries)

    def __str__(self):
        """Returns description of self"""
        return 'RedisQueueService with host %s, port %s, db %s, password=%s...' % (self.host, self.port, self.db, self.password[:3] if self.password else None)

    def resetredis(self, retries=-1):
        """Sets or resets the redis connection.
        Keeps retrying given number of times (<0 means infinitely), using self.timeout as a delay"""
        while retries != 0:
            self.redis = redis.Redis(host=self.host, port=self.port, db=self.db, password=self.password, charset='ascii', socket_timeout=self.socket_timeout)
            # make sure the connection is working
            try:
                x = self.redis.dbsize()
                break
            except redis.exceptions.ConnectionError:
                retries -= 1
                time.sleep(self.timeout)

    def encode(self, obj):
        """Encodes the given object.
        Uses our chosen encoder"""
        if self.encoder == 'pickle':
            import cPickle as pickle
            return pickle.dumps(obj, -1)
        elif self.encoder == 'json':
            try:
                import simplejson as json
            except ImportError:
                import json
            return json.dumps(obj)

    def decode(self, s):
        """Decodes an object from the given string"""
        if self.encoder == 'pickle':
            import cPickle as pickle
            return pickle.loads(s)
        elif self.encoder == 'json':
            try:
                import simplejson as json
            except ImportError:
                import json
            return json.loads(s)

    def getqueue(self, zsname, incr=-1, max=1, retries=-1):
        """Gets a queue to use based on a zset.
        Chooses the item with the maximum score if max=1 (default), else the minimum score.
        If incr is != 0 (default -1), then increments the priority of the chosen job with it.
        Returns (item, score), with the old score (prior to incrementing).
        If there were no items, or an error, returns None."""
        while retries != 0:
            try:
                ret = self.redis.zrange(zsname, 0, 0, desc=max, withscores=1)
                #log('For %s, Got a ret of %s' % (zsname, ret,))
                if not ret: return ret
                item, score = ret[0]
                if incr:
                    #log('In getqeuue, incrby %s %s %s' % (zsname, item, incr))
                    self.redis.zincrby(zsname, item, incr)
                return (item, score)
            except redis.exceptions.ConnectionError:
                self.resetredis(retries)
                retries -= 1

    def get(self, qname, callback=None, workname=None, popwork=1, timeout=-1, retries=-1, retorig=0):
        """Gets an item from the given qname in a robust way.
        This involves:
            - getting the object from the queue, optionally putting it on a working queue atomically
            - decoding it to get an (id, item) pair
            - optionally calling a callback with (id, item, qname, rqs)
            - optionally removing it from working queue if it was added there (if popwork=1, which is the default)
            - returning an (id, item) tuple

        By default, this is a non-blocking call (timeout < 0), meaning that it returns None
        if there was no object ready.
        You can make it blocking by setting timeout >= 0:
            timeout=0: blocks forever
            timeout>0: blocks for given number of seconds (cast into an int), then returns result or None on error.
        Blocking catches connection errors and retries again.
        The number of retries can be given, and is infinite (< 0) by default
        If retorig is true, then returns ((id, item), originalobj). This is useful for manually removing items from the work queue later.
        """
        obj = None
        while retries != 0:
            try:
                if timeout < 0:
                    # non-blocking
                    if workname:
                        obj = self.redis.rpoplpush(qname, workname)
                    else:
                        obj = self.redis.rpop(qname)
                    break
                else:
                    # blocking
                    if workname:
                        obj = self.redis.brpoplpush(qname, workname, int(timeout))
                    else:
                        obj = self.redis.brpop(qname, int(timeout))
                    break
            except redis.exceptions.ConnectionError:
                self.resetredis(retries)
                retries -= 1
        # at this point, if there was no object, return None
        if not obj: return obj
        #log('@@ Got obj %s of type %s' % (obj, type(obj)))
        # there was an object, so decode it
        id, item = self.decode(obj)
        if callback:
            callback(id, item, qname, self)
            #x = self.redis.srem(hashname, id)
            #log('** Removing %s from set %s and got %s' % (id, hashname, x))
        # remove it from working queue if we added it there and we want to remove it
        if workname and popwork:
            x = self.redis.lrem(workname, obj, num=1)
            #log('## Removing %s from workq %s and got %s' % (obj, workname, x))
        if retorig: return ((id, item), obj)
        return (id, item)

    def getmany(self, qname, func, num=-1, **kw):
        """Gets and processes many items from the given input queue.
        The num determines how many items to get:
            < 0: used as a total time limit for amount of time spent getting from queue
            = 0: until the queue is empty
            > 0: maximum number of items to get from the queue
        """
        # first get a bunch of items
        todo = []
        origs = []
        t1 = time.time()
        while 1:
            # try to get an item
            obj = self.get(qname, popwork=0, retorig=1, **kw)
            if not obj: break # no more items
            obj, orig = obj
            todo.append(obj)
            origs.append(orig)
            if num > 0: # max number of items
                if len(todo) > num: break
            elif num < 0: # maximum amount of time
                if time.time() - t1 > abs(num): break
        # now process
        ret = func(todo)
        # remove from workingoutq
        if 'workname' in kw:
            for obj in origs:
                x = self.redis.lrem(kw['workname'], obj, num=1)
        # return results
        return ret

    def put(self, id, item, qname, callback=None, retries=-1):
        """Puts an item on the given qname in a robust way.
        This involves:
            - encoding the (id, item) into an obj
            - optionally calling a callback with (id, item, qname, rqs)
            - putting the obj on the given output queue
        The number of retries can be given, and is infinite (< 0) by default
        """
        obj = self.encode((id, item))
        while retries != 0:
            try:
                x = self.redis.lpush(qname, obj)
                #log('## putting %s on %s and got %s' % (obj, qname, x))
                if callback:
                    callback(id, item, qname, self)
                break
            except redis.exceptions.ConnectionError:
                self.resetredis(retries)
                retries -= 1

    def putmany(self, ids, items, qname, callback=None, retries=-1):
        """Puts many items on the given qname in a robust way.
        This involves:
            - encoding each (id, item) into an obj
            - optionally calling a callback with (id, item, qname, rqs)
            - putting each obj on the given output queue
        The number of retries can be given, and is infinite (< 0) by default
        """
        objs = [self.encode((id, item)) for id, item in zip(ids, items)]
        p = self.redis.pipeline()
        for obj, id, item in zip(objs, ids, items):
            while retries != 0:
                try:
                    x = p.lpush(qname, obj)
                    #log('## putting %s on %s and got %s' % (obj, qname, x))
                    if callback:
                        callback(id, item, qname, self)
                    break
                except redis.exceptions.ConnectionError:
                    self.resetredis(retries)
                    retries -= 1
        p.execute()

    def setstatusmsg(self, hashname, hashkey, msg, pipeline=None):
        """Sets the status message.
        Right now, it calls redis.hset(hashname, hashkey, '%s_%s' % (msg, timestamp)) unless
        pipeline is set, in which case it calls that on the pipeline.
        """
        p = pipeline if pipeline else self.redis.pipeline()
        p.hset(hashname, hashkey, '%s_%s' % (msg, str(time.time())))
        if not pipeline:
            p.execute()


from Queue import Full, Empty
class RQSQueue(object):
    """An abstraction over RedisQueueService queues, with the python Queue.Queue interface.
    This is not just a simple wrapper over Redis' list datatype, but a more sophisticated
    interface which includes our conventions for working with different jobs.
    """
    def __init__(self, inq=None, outq=None, incr=5, get_callback=None, put_callback=None, rqs=None, **rqs_kw):
        """Initializes this rqs queue object with the given rqs keywords.
        You can set the inq and/or the outq with the basenames for the input and output queues.
        These are assumed to end in :inq and :outq, respectively.
        They should be implemented as zsets in redis, and contain jobids.
        The appropriate job queue is gotten by appending :<jobid> to the qname.
        These job queues should be redis lists.
        The incr determines how many items to get from a single job queue before
        moving on and checking the next queue.
        The get_callback and put_callback functions are sent to the underlying rqs.get() and rqs.put() calls.
        """
        if rqs:
            self.rqs = rqs
        else:
            self.rqs = RedisQueueService(**rqs_kw)
        self.retries = rqs_kw.get('retries', -1)
        self.inq, self.outq = inq, outq
        if self.inq: assert self.inq.endswith(':inq')
        if self.outq: assert self.outq.endswith(':outq')
        self.incr = incr
        self.get_callback = get_callback
        self.put_callback = put_callback
        # set some instance variables
        self.curincr = 0
        self.jobid = None

    def __str__(self):
        """Returns description of self"""
        return 'RQSQueue with inq %s, outq %s and rqs %s' % (self.inq, self.outq, str(self.rqs))

    def qsize(self):
        """Returns approximate queue size"""
        return 5 #FIXME this is just a placeholder

    def get(self, blocking=True, timeout=0):
        """Gets an item from the input queue.
        The item will be a ((realid, jobid), item) pair.
        To handle blocking and timeout, we do the following:
            If not blocking, then cycle through all jobs once, with negative timeout on each.
            If blocking with finite timeout (int, > 0), then cycle through jobs with the given timeouts on EACH job.
        Return as soon as you get an item.
        """
        tried = set()
        nloops = 0
        while 1:
            #log('At top of loop, with curincr %s, nloops %s, jobid %s' % (self.curincr, nloops, self.jobid))
            nloops += 1
            if self.curincr == 0 or self.jobid is None:
                # get a new job to work on
                cur = self.rqs.getqueue(self.inq, incr=-1, max=1, retries=self.retries)
                if not cur: raise Empty # nothing todo right now
                # if we're here, then we have a jobid
                self.jobid = cur[0]
                if self.jobid in tried: raise Empty # we already tried to get an item from this job, and failed
            # try to get an item from the currently selected job
            q = ':'.join((self.inq, self.jobid))
            if not blocking or timeout < 0: # don't block on this individual queue
                curtimeout = -1
            if blocking and timeout >= 0:
                curtimeout = timeout
            obj = self.rqs.get(q, callback=self.get_callback, workname=q.replace(':inq', ':inworkq'), timeout=curtimeout)
            #log('In get, with nloops=%d, jobid %s, obj %s' % (nloops, self.jobid, obj))
            if not obj:
                # empty/error, so immediately try to get a new jobid
                tried.add(self.jobid)
                self.curincr = 0
                continue
            # we got an item, so wrap the id into (id, jobid)
            id, item = obj
            # increment counters
            self.curincr += 1
            if self.curincr > self.incr:
                self.curincr = 0
            # return the object
            return ((id, self.jobid), item)

    def put(self, item, blocking=True, timeout=0):
        """Puts an item on the queue.
        The item must be a ((realid, jobid), actualitem) pair.
        The actual item must be a tuple and will have its first element replaced by the realid.
        We ignore the blocking and timeout parameters.
        """
        id, item = item
        #print 'Unpacked into id %s, item %s' % (id, item)
        # unpack the id into the original "realid" and the jobid, which we tacked onto it
        realid, jobid = id
        #print 'Got realid %s, jobid %s' % (realid, jobid)
        item = (realid,)+ tuple(item[1:])
        #print 'Got new item %s' % (item,)
        q = ':'.join((self.outq, str(jobid)))
        #log('  Putting %s, %s on outq %s' % (realid, item, q))
        self.rqs.put(realid, item, q, callback=self.put_callback, retries=self.retries)

    def putmany(self, items, blocking=True, timeout=0):
        """Puts many items on the queue.
        Each item must be a ((realid, jobid), actualitem) pair.
        The actual item must be a tuple and will have its first element replaced by the realid.
        We ignore the blocking and timeout parameters.
        """
        toput = {}
        # build up list of things toput, separated by qname
        for item in items:
            id, item = item
            #print 'Unpacked into id %s, item %s' % (id, item)
            # unpack the id into the original "realid" and the jobid, which we tacked onto it
            realid, jobid = id
            #print 'Got realid %s, jobid %s' % (realid, jobid)
            item = (realid,)+ tuple(item[1:])
            #print 'Got new item %s' % (item,)
            q = ':'.join((self.outq, jobid))
            #log('  Putting %s, %s on outq %s' % (realid, item, q))
            toput.setdefault(q, []).append((realid, item))
        # put all items by qname
        for q, fullitems in toput:
            realids, items = zip(*fullitems)
            self.rqs.putmany(realids, items, q, callback=self.put_callback, retries=self.retries)

    def get_nowait(self):
        """Non-blocking get"""
        return self.get(blocking=0)

    def put_nowait(self, item):
        """Non-blocking put"""
        return self.put(item, blocking=0)

MAINDB_PATTERNS = [
    'active_jobs',
    'active_jobs:*',
    'archived',
    'archived:*',
    'deletedjobs',
    'errors:*:*',
    'face_counter',
    'faces:*',
    'fintimes:*',
    'image_counter',
    'images:*:*',
    'images:*',
    'jobs',
    'jobs:*:facemap',
    'jobs:*:faces',
    'jobs:*:images',
    'jobs:*:results',
    'jobs:*:status',
    'jobs:*:tasks',
    'jobs:*',
    'resets:*:*',
    'resettimes:*',
    'results:json:*',
    'temp_sub_*:*',
    'todo_*:*',
    'users:*:archived',
    'users:*:jobs',
    'users:*',
    'user_counter',
    'usermap',
    'users',
]

RQS_PATTERNS = [
    'faceservice:afs:clspriorities',
    'faceservice:afs:clssizes',
    'faceservice:afs:jobpriorities',
    'faceservice:afs:jobsizes',
    'faceservice:afs:perc_small',
    'perc_small',
    'rqs:*:inq',
    'rqs:*:inq:*',
    'rqs:*:inworkq:*',
    'rqs:*:outq:*',
    'rqs:*:outworkq:*',
    'rqs:*:status:*',
    'rqs:*:tempstatus:*',
    'rqs:*:userinq:*',
    'rqs:*:workers',
    'rqs:*:workers:*',
]

def groupingmain(db):
    """Program for figuring out how to group a database"""
    patterns = RQS_PATTERNS
    if 0:
        keys = db.keys()
        for k in keys:
            print k
        sys.exit()
    else:
        keys = [l.strip() for l in open('rqskeys')]
    print '%d keys' % (len(keys))
    groups = groupkeys(keys, patterns)
    for pat in patterns:
        print '%8d\t%s' % (len(groups[pat]), pat)
    print '%8d\tNone' % (len(groups.get(None, [])))
    sys.exit()

def testmain(db):
    """Tests various functions"""
    #keys = [l.strip() for i, l in enumerate(open('rqskeys')) if i < 10000000]
    keys = sorted(db.keys())
    if 0:
        # keymem
        ret = keymem(db, keys)
        pprint.pprint(ret)
        # aggrmem, dict
        aggr = aggrmem(ret)
        pprint.pprint(aggr)
        # aggrmem, list
        aggr = aggrmem(ret.values())
        pprint.pprint(aggr)
        # dbinfo
        info = dbinfo(db, *keys)
        pprint.pprint(info)
        # dblist
        vals = dblist(db, *keys)
        pprint.pprint(vals)
    # groupedmem
    mem = groupedmem(db, keys, MAINDB_PATTERNS)
    pprint.pprint(mem)

if __name__ == '__main__':
    import json
    # initialize the database
    host = 'localhost'
    port = 6379
    db = 0
    password = None
    if len(sys.argv) < 1:
        print 'Usage: python %s [<host=%s> [<port=%s> [<db=%s> [<password=%s>]]]]' % (sys.argv[0], host, port, db, password)
        sys.exit()
    try:
        try:
            # first see if the first arg is a json file with parameters
            j = json.load(open(sys.argv[1]))
            locals().update(j)
        except Exception:
            # otherwise, assume the parameters are in stdin
            host = sys.argv[1]
            port = int(sys.argv[2])
            db = int(sys.argv[3])
            password = sys.argv[4]
    except Exception: pass
    r = redis.Redis(host=host, port=port, db=db, password=password)
    # init done, now run what you want
    #testmain(r); sys.exit()
    def i(*pat):
        x = dbinfo(r, *pat)

    def l(*pat):
        x = dblist(r, *pat)

    lev = lambda num: keysAtLevel(r, num)

    def I(*pats):
        ret = r.info()
        def matches(s):
            for pat in pats:
                if pat in s: return 1
            return 0

        if pats:
            validkeys = [k for k in ret if matches(k)]
            ret = dict((k, ret[k]) for k in validkeys)
        pprint(ret)

    reali, reall, reallev, realI = i, l, lev, I # as backups
    re = repl(locals=locals())
    re.run()
